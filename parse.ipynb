{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "b94b3d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib as plt\n",
    "import cv2\n",
    "filepath = \"/Users/natejly/Desktop/sorted_digits\"\n",
    "# filepath = \"/Users/natejly/Desktop/chest_xray\"\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Sequence, Tuple, Callable\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "ae02efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate folder tree as JSON for LLM input, including file count and folder count for each folder, and collect leaf files\n",
    "def folder_tree_json(path):\n",
    "    leaf_files = []\n",
    "    path = Path(path)\n",
    "    def build_tree(p):\n",
    "        temp_leaf_files = []\n",
    "        children = [c for c in p.iterdir() if c.is_dir()]\n",
    "        is_leaf = len(children) == 0\n",
    "        folder_count = len(children)\n",
    "        if is_leaf:\n",
    "            for f in p.iterdir():\n",
    "                if f.is_file():\n",
    "                    temp_leaf_files.append(str(f))\n",
    "            file_count = len(temp_leaf_files)\n",
    "            leaf_files.extend(temp_leaf_files)\n",
    "        else:\n",
    "            file_count = None\n",
    "        return {\n",
    "            \"folder_name\": p.name,\n",
    "            \"is_leaf\": is_leaf,\n",
    "            \"file_count\": file_count,\n",
    "            \"folder_count\": folder_count,\n",
    "            \"sub_folders\": [build_tree(child) for child in children]\n",
    "        }\n",
    "    tree = build_tree(path)\n",
    "    return tree, leaf_files\n",
    "def get_image_size(leaf_files, sample_rate=50):\n",
    "    sampled_files = leaf_files[::sample_rate]\n",
    "    sizes = []\n",
    "    pxtodim = {}\n",
    "    for file in sampled_files:\n",
    "        with Image.open(file) as img:\n",
    "            pixels = 1\n",
    "            for dim in img.size:\n",
    "                pixels *= dim\n",
    "            pxtodim[pixels] = img.size\n",
    "            sizes.append(pixels)\n",
    "\n",
    "    median_size = np.median(sizes)\n",
    "    return pxtodim.get(median_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "7e7546ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"folder_name\": \"sorted_digits\",\n",
      "  \"is_leaf\": false,\n",
      "  \"file_count\": null,\n",
      "  \"folder_count\": 13,\n",
      "  \"sub_folders\": [\n",
      "    {\n",
      "      \"folder_name\": \"test\",\n",
      "      \"is_leaf\": false,\n",
      "      \"file_count\": null,\n",
      "      \"folder_count\": 10,\n",
      "      \"sub_folders\": [\n",
      "        {\n",
      "          \"folder_name\": \"9\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 152,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"0\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 147,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"7\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 155,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"6\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 145,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"1\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 171,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"8\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 147,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"4\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 148,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"3\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 152,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"2\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 156,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"5\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 135,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"folder_name\": \"9\",\n",
      "      \"is_leaf\": true,\n",
      "      \"file_count\": 0,\n",
      "      \"folder_count\": 0,\n",
      "      \"sub_folders\": []\n",
      "    },\n",
      "    {\n",
      "      \"folder_name\": \"0\",\n",
      "      \"is_leaf\": true,\n",
      "      \"file_count\": 0,\n",
      "      \"folder_count\": 0,\n",
      "      \"sub_folders\": []\n",
      "    },\n",
      "    {\n",
      "      \"folder_name\": \"7\",\n",
      "      \"is_leaf\": true,\n",
      "      \"file_count\": 0,\n",
      "      \"folder_count\": 0,\n",
      "      \"sub_folders\": []\n",
      "    },\n",
      "    {\n",
      "      \"folder_name\": \"6\",\n",
      "      \"is_leaf\": true,\n",
      "      \"file_count\": 0,\n",
      "      \"folder_count\": 0,\n",
      "      \"sub_folders\": []\n",
      "    },\n",
      "    {\n",
      "      \"folder_name\": \"1\",\n",
      "      \"is_leaf\": true,\n",
      "      \"file_count\": 0,\n",
      "      \"folder_count\": 0,\n",
      "      \"sub_folders\": []\n",
      "    },\n",
      "    {\n",
      "      \"folder_name\": \"8\",\n",
      "      \"is_leaf\": true,\n",
      "      \"file_count\": 0,\n",
      "      \"folder_count\": 0,\n",
      "      \"sub_folders\": []\n",
      "    },\n",
      "    {\n",
      "      \"folder_name\": \"train\",\n",
      "      \"is_leaf\": false,\n",
      "      \"file_count\": null,\n",
      "      \"folder_count\": 10,\n",
      "      \"sub_folders\": [\n",
      "        {\n",
      "          \"folder_name\": \"9\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 706,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"0\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 686,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"7\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 720,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"6\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 670,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"1\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 794,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"8\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 681,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"4\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 687,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"3\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 707,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"2\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 722,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"5\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 624,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"folder_name\": \"4\",\n",
      "      \"is_leaf\": true,\n",
      "      \"file_count\": 0,\n",
      "      \"folder_count\": 0,\n",
      "      \"sub_folders\": []\n",
      "    },\n",
      "    {\n",
      "      \"folder_name\": \"3\",\n",
      "      \"is_leaf\": true,\n",
      "      \"file_count\": 0,\n",
      "      \"folder_count\": 0,\n",
      "      \"sub_folders\": []\n",
      "    },\n",
      "    {\n",
      "      \"folder_name\": \"2\",\n",
      "      \"is_leaf\": true,\n",
      "      \"file_count\": 0,\n",
      "      \"folder_count\": 0,\n",
      "      \"sub_folders\": []\n",
      "    },\n",
      "    {\n",
      "      \"folder_name\": \"5\",\n",
      "      \"is_leaf\": true,\n",
      "      \"file_count\": 0,\n",
      "      \"folder_count\": 0,\n",
      "      \"sub_folders\": []\n",
      "    },\n",
      "    {\n",
      "      \"folder_name\": \"val\",\n",
      "      \"is_leaf\": false,\n",
      "      \"file_count\": null,\n",
      "      \"folder_count\": 10,\n",
      "      \"sub_folders\": [\n",
      "        {\n",
      "          \"folder_name\": \"9\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 151,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"0\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 147,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"7\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 154,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"6\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 143,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"1\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 170,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"8\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 146,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"4\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 147,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"3\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 151,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"2\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 154,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        },\n",
      "        {\n",
      "          \"folder_name\": \"5\",\n",
      "          \"is_leaf\": true,\n",
      "          \"file_count\": 133,\n",
      "          \"folder_count\": 0,\n",
      "          \"sub_folders\": []\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "#sample every 1/20th file in the leaf files\n",
    "\n",
    "tree_json, leaf_files = folder_tree_json(filepath)\n",
    "img_dims = get_image_size(leaf_files, sample_rate=100)\n",
    "print(json.dumps(tree_json, indent=2))\n",
    "print(img_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "36eac26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"train_test_val_split_exists\": true,\n",
      "  \"splits\": {\n",
      "    \"test\": {\n",
      "      \"9\": \"/Users/natejly/Desktop/sorted_digits/test/9\",\n",
      "      \"0\": \"/Users/natejly/Desktop/sorted_digits/test/0\",\n",
      "      \"7\": \"/Users/natejly/Desktop/sorted_digits/test/7\",\n",
      "      \"6\": \"/Users/natejly/Desktop/sorted_digits/test/6\",\n",
      "      \"1\": \"/Users/natejly/Desktop/sorted_digits/test/1\",\n",
      "      \"8\": \"/Users/natejly/Desktop/sorted_digits/test/8\",\n",
      "      \"4\": \"/Users/natejly/Desktop/sorted_digits/test/4\",\n",
      "      \"3\": \"/Users/natejly/Desktop/sorted_digits/test/3\",\n",
      "      \"2\": \"/Users/natejly/Desktop/sorted_digits/test/2\",\n",
      "      \"5\": \"/Users/natejly/Desktop/sorted_digits/test/5\"\n",
      "    },\n",
      "    \"train\": {\n",
      "      \"9\": \"/Users/natejly/Desktop/sorted_digits/train/9\",\n",
      "      \"0\": \"/Users/natejly/Desktop/sorted_digits/train/0\",\n",
      "      \"7\": \"/Users/natejly/Desktop/sorted_digits/train/7\",\n",
      "      \"6\": \"/Users/natejly/Desktop/sorted_digits/train/6\",\n",
      "      \"1\": \"/Users/natejly/Desktop/sorted_digits/train/1\",\n",
      "      \"8\": \"/Users/natejly/Desktop/sorted_digits/train/8\",\n",
      "      \"4\": \"/Users/natejly/Desktop/sorted_digits/train/4\",\n",
      "      \"3\": \"/Users/natejly/Desktop/sorted_digits/train/3\",\n",
      "      \"2\": \"/Users/natejly/Desktop/sorted_digits/train/2\",\n",
      "      \"5\": \"/Users/natejly/Desktop/sorted_digits/train/5\"\n",
      "    },\n",
      "    \"val\": {\n",
      "      \"9\": \"/Users/natejly/Desktop/sorted_digits/val/9\",\n",
      "      \"0\": \"/Users/natejly/Desktop/sorted_digits/val/0\",\n",
      "      \"7\": \"/Users/natejly/Desktop/sorted_digits/val/7\",\n",
      "      \"6\": \"/Users/natejly/Desktop/sorted_digits/val/6\",\n",
      "      \"1\": \"/Users/natejly/Desktop/sorted_digits/val/1\",\n",
      "      \"8\": \"/Users/natejly/Desktop/sorted_digits/val/8\",\n",
      "      \"4\": \"/Users/natejly/Desktop/sorted_digits/val/4\",\n",
      "      \"3\": \"/Users/natejly/Desktop/sorted_digits/val/3\",\n",
      "      \"2\": \"/Users/natejly/Desktop/sorted_digits/val/2\",\n",
      "      \"5\": \"/Users/natejly/Desktop/sorted_digits/val/5\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def analyze_dataset_structure_and_splits(tree_json, filepath):\n",
    "    \"\"\"\n",
    "    Analyze dataset structure to detect train/test/val splits and organize folder paths\n",
    "    \"\"\"\n",
    "    # Check if dataset has train/test/val split structure\n",
    "    split_keywords = ['train', 'test', 'val', 'validation', 'dev']\n",
    "    \n",
    "    def has_split_structure(node):\n",
    "        folder_names = [child['folder_name'].lower() for child in node.get('sub_folders', [])]\n",
    "        return any(keyword in name for keyword in split_keywords for name in folder_names)\n",
    "    \n",
    "    has_splits = has_split_structure(tree_json)\n",
    "    \n",
    "    # Organize file paths\n",
    "    dataset_splits = {\n",
    "        \"train_test_val_split_exists\": has_splits,\n",
    "        \"splits\": {}\n",
    "    }\n",
    "    \n",
    "    if has_splits:\n",
    "        # Organize by detected splits\n",
    "        def organize_by_splits(node, current_path=\"\"):\n",
    "            for child in node.get('sub_folders', []):\n",
    "                child_name = child['folder_name'].lower()\n",
    "                path_key = f\"{current_path}/{child['folder_name']}\" if current_path else child['folder_name']\n",
    "                \n",
    "                # Determine split type\n",
    "                split_type = None\n",
    "                if 'train' in child_name:\n",
    "                    split_type = 'train'\n",
    "                elif 'test' in child_name:\n",
    "                    split_type = 'test'\n",
    "                elif 'val' in child_name or 'validation' in child_name:\n",
    "                    split_type = 'val'\n",
    "                \n",
    "                if split_type and child.get('is_leaf'):\n",
    "                    # Leaf folder with files\n",
    "                    if split_type not in dataset_splits[\"splits\"]:\n",
    "                        dataset_splits[\"splits\"][split_type] = {}\n",
    "                    \n",
    "                    class_name = child['folder_name']\n",
    "                    if current_path:\n",
    "                        folder_path = f\"{filepath}/{current_path}/{child['folder_name']}\"\n",
    "                    else:\n",
    "                        folder_path = f\"{filepath}/{child['folder_name']}\"\n",
    "                    dataset_splits[\"splits\"][split_type][class_name] = folder_path\n",
    "                \n",
    "                elif split_type and not child.get('is_leaf'):\n",
    "                    # Split folder with class subfolders\n",
    "                    if split_type not in dataset_splits[\"splits\"]:\n",
    "                        dataset_splits[\"splits\"][split_type] = {}\n",
    "                    \n",
    "                    for class_folder in child.get('sub_folders', []):\n",
    "                        if class_folder.get('is_leaf'):\n",
    "                            class_name = class_folder['folder_name']\n",
    "                            folder_path = f\"{filepath}/{path_key}/{class_name}\"\n",
    "                            dataset_splits[\"splits\"][split_type][class_name] = folder_path\n",
    "                \n",
    "                # Recursively check subfolders\n",
    "                organize_by_splits(child, path_key)\n",
    "        \n",
    "        organize_by_splits(tree_json)\n",
    "    else:\n",
    "        # No splits detected - put all files in training set\n",
    "        dataset_splits[\"splits\"][\"train\"] = {}\n",
    "        \n",
    "        def organize_all_as_train(node, current_path=\"\"):\n",
    "            if node.get('is_leaf'):\n",
    "                class_name = node['folder_name']\n",
    "                if current_path:\n",
    "                    folder_path = f\"{filepath}/{current_path}\"\n",
    "                else:\n",
    "                    folder_path = f\"{filepath}\"\n",
    "                dataset_splits[\"splits\"][\"train\"][class_name] = folder_path\n",
    "            \n",
    "            for child in node.get('sub_folders', []):\n",
    "                child_path = f\"{current_path}/{child['folder_name']}\" if current_path else child['folder_name']\n",
    "                organize_all_as_train(child, child_path)\n",
    "        \n",
    "        organize_all_as_train(tree_json)\n",
    "    \n",
    "    return dataset_splits\n",
    "\n",
    "dataset_splits = analyze_dataset_structure_and_splits(tree_json, filepath)\n",
    "\n",
    "print(json.dumps(dataset_splits, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "d1a20579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def split_dataset(parent_folder, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    # Ensure ratios sum to 1\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1\"\n",
    "\n",
    "    # Create output folders\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        split_path = os.path.join(parent_folder, split)\n",
    "        os.makedirs(split_path, exist_ok=True)\n",
    "\n",
    "    # Loop through each class folder inside parent\n",
    "    for class_name in os.listdir(parent_folder):\n",
    "        class_path = os.path.join(parent_folder, class_name)\n",
    "        \n",
    "        # Skip the folders we just created\n",
    "        if class_name in [\"train\", \"val\", \"test\"]:\n",
    "            continue\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        # Collect all files for this class\n",
    "        files = os.listdir(class_path)\n",
    "        random.shuffle(files)\n",
    "\n",
    "        # Split indices\n",
    "        n_total = len(files)\n",
    "        n_train = int(train_ratio * n_total)\n",
    "        n_val = int(val_ratio * n_total)\n",
    "\n",
    "        train_files = files[:n_train]\n",
    "        val_files = files[n_train:n_train + n_val]\n",
    "        test_files = files[n_train + n_val:]\n",
    "\n",
    "        # Function to move files into split folders\n",
    "        def move_files(file_list, split):\n",
    "            split_class_dir = os.path.join(parent_folder, split, class_name)\n",
    "            os.makedirs(split_class_dir, exist_ok=True)\n",
    "            for f in file_list:\n",
    "                src = os.path.join(class_path, f)\n",
    "                dst = os.path.join(split_class_dir, f)\n",
    "                shutil.move(src, dst)\n",
    "\n",
    "        # Move files\n",
    "        move_files(train_files, \"train\")\n",
    "        move_files(val_files, \"val\")\n",
    "        move_files(test_files, \"test\")\n",
    "    train_dir = os.path.join(parent_folder, \"train\")\n",
    "    val_dir = os.path.join(parent_folder, \"val\")\n",
    "    test_dir = os.path.join(parent_folder, \"test\")\n",
    "\n",
    "    return train_dir, val_dir, test_dir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a45718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_splits(ds_split_tree):\n",
    "    #TODO: Implement train test split\n",
    "    data = json\n",
    "    train_folders = []\n",
    "    val_folders = []\n",
    "    test_folders = []\n",
    "    # organize the data into splits\n",
    "    has_splits = data[\"train_test_val_split_exists\"]\n",
    "    if has_splits:\n",
    "        if \"train\" in data[\"splits\"]:\n",
    "            for value in data[\"splits\"][\"train\"].values():\n",
    "                train_folders.append(value)\n",
    "        if \"val\" in data[\"splits\"]:\n",
    "            for value in data[\"splits\"][\"val\"].values():\n",
    "                val_folders.append(value)\n",
    "        if \"test\" in data[\"splits\"]:\n",
    "            for value in data[\"splits\"][\"test\"].values():\n",
    "                test_folders.append(value)\n",
    "    else:\n",
    "        return split_dataset(filepath)\n",
    "\n",
    "    return train_folders, val_folders, test_folders\n",
    "\n",
    "train_folders, val_folders, test_folders = gen_splits(dataset_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "13b8ba3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dir: /Users/natejly/Desktop/sorted_digits/train\n",
      "val dir: /Users/natejly/Desktop/sorted_digits/val\n",
      "test dir: /Users/natejly/Desktop/sorted_digits/test\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "train_dir = os.path.dirname(train_folders[0])\n",
    "val_dir = os.path.dirname(val_folders[0]) \n",
    "test_dir = os.path.dirname(test_folders[0]) \n",
    "print(\"train dir:\", train_dir)\n",
    "print(\"val dir:\", val_dir)\n",
    "print(\"test dir:\", test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "3a96ee25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected classes: 10\n",
      "Using image size: (32, 32)\n",
      "Found 6996 files belonging to 10 classes.\n",
      "Found 1496 files belonging to 10 classes.\n",
      "Found 1508 files belonging to 10 classes.\n",
      "Total parameters: 4,062,381\n",
      "Trainable parameters: 12,810\n",
      "\n",
      "=== STAGE 1: Training with frozen backbone ===\n",
      "Epoch 1/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 41ms/step - accuracy: 0.3063 - loss: 2.0233 - val_accuracy: 0.6604 - val_loss: 1.4584\n",
      "Epoch 2/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.6026 - loss: 1.4236 - val_accuracy: 0.6878 - val_loss: 1.1945\n",
      "Epoch 3/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - accuracy: 0.6704 - loss: 1.1939 - val_accuracy: 0.7447 - val_loss: 1.0432\n",
      "Epoch 4/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.6976 - loss: 1.0740 - val_accuracy: 0.7600 - val_loss: 0.9540\n",
      "Epoch 5/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7125 - loss: 0.9989 - val_accuracy: 0.7721 - val_loss: 0.8864\n",
      "Epoch 6/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7302 - loss: 0.9305 - val_accuracy: 0.7821 - val_loss: 0.8426\n",
      "Epoch 7/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.7335 - loss: 0.8871 - val_accuracy: 0.7774 - val_loss: 0.8047\n",
      "Epoch 8/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.7484 - loss: 0.8571 - val_accuracy: 0.7874 - val_loss: 0.7647\n",
      "Epoch 9/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7617 - loss: 0.8109 - val_accuracy: 0.8001 - val_loss: 0.7320\n",
      "Epoch 10/10\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step - accuracy: 0.7552 - loss: 0.7902 - val_accuracy: 0.8048 - val_loss: 0.7110\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "def get_classes_from_train(train_dir):\n",
    "    return len([d.name for d in os.scandir(train_dir) if d.is_dir()])\n",
    "#TODO: figure out image scaling\n",
    "def bucket_dims(img_dims):\n",
    "    if img_dims[0] < 128:\n",
    "        dim = max(32, max(img_dims[0], img_dims[1]))\n",
    "        return (dim, dim)\n",
    "    if img_dims[0] < 256:\n",
    "        return (128, 128)\n",
    "    elif img_dims[0] < 512:\n",
    "        return (256, 256)\n",
    "    else:\n",
    "        return (512, 512)\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------\n",
    "IMG_SIZE = bucket_dims(img_dims)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "NUM_CLASSES = get_classes_from_train(train_dir)\n",
    "print(\"Detected classes:\", NUM_CLASSES)\n",
    "print(\"Using image size:\", IMG_SIZE)\n",
    "\n",
    "BASE_MODEL_NAME = \"EfficientNetB0\"\n",
    "INITIAL_LEARNING_RATE = 1e-3  # Higher LR for frozen backbone\n",
    "FINE_TUNE_LEARNING_RATE = 1e-5  # Lower LR for fine-tuning\n",
    "\n",
    "# -----------------------------\n",
    "# FUNCTION: Load Base Model\n",
    "# -----------------------------\n",
    "def get_base_model(model_name, input_shape, weights=\"imagenet\"):\n",
    "    \"\"\"\n",
    "    Returns a pretrained model (EfficientNet variants) without the top layer.\n",
    "    \"\"\"\n",
    "    ModelClass = getattr(tf.keras.applications, model_name)\n",
    "    base_model = ModelClass(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=input_shape + (3,)\n",
    "    )\n",
    "    base_model.trainable = False  # Start with frozen backbone\n",
    "    return base_model\n",
    "\n",
    "def build_model(base_model, num_classes):\n",
    "    inputs = tf.keras.Input(shape=IMG_SIZE + (3,))\n",
    "    x = base_model(inputs, training=False)         # backbone\n",
    "    x = layers.GlobalAveragePooling2D()(x)         # flatten features\n",
    "    x = layers.Dropout(0.2)(x)                     # regularization\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "# -----------------------------\n",
    "# DATA PREPROCESSING\n",
    "# -----------------------------\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = preprocess_input(image)   # scale to [-1,1]\n",
    "    return image, label\n",
    "\n",
    "# Data augmentation for training (optional but recommended)\n",
    "def augment(image, label):\n",
    "    return image, label\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir, image_size=IMG_SIZE, batch_size=BATCH_SIZE, shuffle=True\n",
    ").map(preprocess, num_parallel_calls=AUTOTUNE).map(augment, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    val_dir, image_size=IMG_SIZE, batch_size=BATCH_SIZE, shuffle=False\n",
    ").map(preprocess, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir, image_size=IMG_SIZE, batch_size=BATCH_SIZE, shuffle=False\n",
    ").map(preprocess, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\n",
    "\n",
    "# -----------------------------\n",
    "# BUILD AND COMPILE MODEL\n",
    "# -----------------------------\n",
    "base_model = get_base_model(BASE_MODEL_NAME, IMG_SIZE)\n",
    "model = build_model(base_model, NUM_CLASSES)\n",
    "\n",
    "print(f\"Total parameters: {model.count_params():,}\")\n",
    "print(f\"Trainable parameters: {sum([tf.keras.backend.count_params(w) for w in model.trainable_weights]):,}\")\n",
    "# -----------------------------\n",
    "# STAGE 1: Train with frozen backbone\n",
    "# -----------------------------\n",
    "base_model.trainable = False\n",
    "print(\"\\n=== STAGE 1: Training with frozen backbone ===\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(INITIAL_LEARNING_RATE),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "callbacks_stage1 = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=2,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "\n",
    "history_stage1 = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,   # effectively \"infinite\"\n",
    "    callbacks=callbacks_stage1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# STAGE 2: Fine-tune with unfrozen backbone\n",
    "# -----------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106ff964",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
